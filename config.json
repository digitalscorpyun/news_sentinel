import requests
import feedparser
import csv
import logging
import os
import time
from datetime import datetime
from requests.exceptions import RequestException
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import json

# --- Load Configuration ---
with open("config.json", "r") as config_file:
    CONFIG = json.load(config_file)

# --- Configure Logging ---
log_filename = f"news_sentinel_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    filename=log_filename,
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# Add console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
logging.getLogger().addHandler(console_handler)

# --- Initialize Selenium WebDriver ---
def initialize_webdriver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=chrome_service, options=chrome_options)

# --- Helper Functions ---
def fetch_rss_feed(url, retries=3, backoff_factor=2):
    """Fetch articles from an RSS feed with retries."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
    }
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            feed = feedparser.parse(response.text)
            if not feed.bozo:
                return feed.entries
        except Exception as e:
            logging.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
            time.sleep(backoff_factor ** attempt)  # Exponential backoff
    logging.error(f"Failed to fetch RSS feed after {retries} attempts: {url}")
    return []

def fetch_dynamic_content(url, source_name):
    """Scrape articles using Selenium."""
    driver = initialize_webdriver()
    articles = []
    try:
        driver.get(url)
        time.sleep(5)  # Allow page to load

        if "semafor" in url:
            article_elements = driver.find_elements(By.CSS_SELECTOR, "article h2 a")
            for elem in article_elements:
                title = elem.text
                link = elem.get_attribute("href")
                articles.append({"title": title, "link": link, "source": source_name})
        else:
            article_elements = driver.find_elements(By.TAG_NAME, "a")
            for elem in article_elements[:10]:  # Limit to first 10 links for performance
                title = elem.text
                link = elem.get_attribute("href")
                if title and link:
                    articles.append({"title": title, "link": link, "source": source_name})
    except Exception as e:
        logging.error(f"Selenium scraping error for {url}: {e}")
    finally:
        driver.quit()
    return articles

def save_to_csv(articles, filename):
    """Save articles to a CSV file."""
    with open(filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["Source", "Link", "Title", "Keywords Used"])
        for article in articles:
            source = article.get("source", "Unknown Source")
            title = article.get("title", "").strip()

            # Create clickable hyperlink
            url = f'=HYPERLINK("{article.get("link", "")}", "Link")'

            # Extract keywords used
            keywords_used = ", ".join(
                [keyword for keyword in CONFIG["KEYWORDS"] if keyword.lower() in title.lower()]
            )

            writer.writerow([source, url, title, keywords_used])

# --- Main Script ---
def main():
    logging.info("News Sentinel started.")
    all_articles = []

    # Fetch articles from RSS feeds
    for source_name, url in CONFIG["RSS_FEEDS"].items():
        logging.info(f"Fetching articles from {source_name}")
        articles = fetch_rss_feed(url)
        if articles:
            for entry in articles:
                all_articles.append({
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "source": source_name
                })
        else:
            logging.warning(f"No articles fetched from {source_name}")

    # Fetch articles dynamically using Selenium
    for source_name, url in CONFIG["WEBSITES"].items():
        logging.info(f"Scraping articles from {source_name}")
        articles = fetch_dynamic_content(url, source_name)
        if articles:
            all_articles.extend(articles)

    # Save to CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"news_{timestamp}.csv"
    save_to_csv(all_articles, output_file)
    logging.info(f"Saved {len(all_articles)} articles to {output_file}")

if __name__ == "__main__":
    main()
